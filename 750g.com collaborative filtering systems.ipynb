{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import datetime as tm\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from tr_utils import *\n",
    "\n",
    "#Téléchargement des cookies du site 750gr sur une semaine\n",
    "adv = 656228\n",
    "dates = get_period('2017-06-07', '2017-06-14')\n",
    "path = 'gs://tr-parquet/its_panel/'\n",
    "dfs = []\n",
    "for date in dates:\n",
    "    dfs += [sqlContext.read.load(path+date+'/ad='+str(adv))]\n",
    "    \n",
    "data = reduce(lambda x, y: x.unionAll(y), dfs)\n",
    "\n",
    "\n",
    "#On récupère les données sous forme d'un DataFrame\n",
    "dl = data.filter((F.col('ty')=='datalayer')&(F.col('id2')!='')&(F.col('id2')!=-1)&(F.col('id2')!=0))\n",
    "#creation d'un schema puis application a l'objet dataLayer\n",
    "test = spark.read.json(dl.limit(1000).rdd.map(lambda x: x.dl))\n",
    "schema = test.schema\n",
    "with open('schema_dl_jv.pickle', 'w') as f:\n",
    "    cPickle.dump(schema, f)\n",
    "    \n",
    "dl_parsed = dl.withColumn('dl2', F.from_json('dl', schema))\n",
    "\n",
    "#Les données les plus importantes sont celles situées dans le dataLayer\n",
    "#On sélectionne les identifiants des users, les recettes consultées ainsi que les ingrédients associés aux recettes\n",
    "#Les ingrédients ne sont pas donnés comme des mots clés, mais avec les quantités associées (sous forme de phrase..)\n",
    "df=dl_parsed.select('dl2.ses.uuid2','dl2.dataLayer.sectionData.averageScore','dl2.dataLayer.sectionData.title'\n",
    "                   ,'dl2.dataLayer.sectionData.ingredients')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "    if not s : \n",
    "        return s\n",
    "    return s.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strip_accents_udf = F.udf(strip_accents,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On regroupe dans une même ligne toutes les données relatives à un user: recettes et ingrédients associés à ces recettes\n",
    "from pyspark.sql.functions import collect_list,concat_ws\n",
    "df2=df.select('uuid2','averageScore',strip_accents_udf('title').alias('title'),strip_accents_udf('ingredients').alias('ingredients')).dropDuplicates().cache()\n",
    "df3= df2.select('uuid2','averageScore',concat_ws('---', df2.title, df2.ingredients).alias('concat'))\n",
    "    \n",
    "grouped = df3.groupBy(['uuid2']).agg(collect_list('concat').alias('concat2'))\n",
    "\n",
    "#Nous séparons notre base deux: la base train et la base test.\n",
    "#Sur la base train, nous modéliserons nos 100 régressions logistiques, et évaluerons la pertinence du modèle sur la base test\n",
    "#Sur la base test, nous ne gardons que 2/3 des recettes, qui nous servent à obtenir les recommendations, et testons \n",
    "#nos recommmendations sur le tiers restant.\n",
    "\n",
    "\n",
    "def lr(u):\n",
    "    l=[]\n",
    "    for i in u.concat2:\n",
    "        l.append(i.split('---')[0])\n",
    "    return l\n",
    "\n",
    "def ir(u):\n",
    "    l=[]\n",
    "    try:\n",
    "        for i in u.concat2:\n",
    "            l.append(i.split('---')[1])\n",
    "        return l\n",
    "    except:\n",
    "        return l\n",
    "\n",
    "rdd=grouped.rdd.map(lambda x: Row(uuid2=x[0], listerecettes2=lr(x),ingredientsrecettes2=ir(x)))\n",
    "grouped = sqlContext.createDataFrame(rdd)\n",
    "\n",
    "def countrecettes(u):\n",
    "    p=set(u)\n",
    "    return float(len(list(p)))\n",
    "\n",
    "countrecettes_=F.udf(countrecettes,FloatType())\n",
    "\n",
    "\n",
    "def enleverdoublons(u):\n",
    "    p=set(u)\n",
    "    return list(p)\n",
    "\n",
    "enleverdoublons_=F.udf(enleverdoublons,ArrayType(StringType()))\n",
    "\n",
    "\n",
    "countgrouped=grouped.select('uuid2',enleverdoublons_('listerecettes2').alias('listerecettes3'),enleverdoublons_('ingredientsrecettes2').alias('ingredientsrecettes3'),'listerecettes2')\n",
    "countgrouped=countgrouped.select('uuid2','listerecettes3','ingredientsrecettes3',countrecettes_('listerecettes3').alias('count'))\n",
    "#Pour notre base test, nous ne sélectionnons que les individus qui ont visionné plus de 3 recettes, \n",
    "#afin d'obtenir les recommendations à l'aide de 2 recettes au minimum, et de vérifier la pertinence des recommendations à l'aide \n",
    "#d'une recette au minimum\n",
    "grouped=countgrouped.where(\"count >= 3.0\")\n",
    "\n",
    "def trainrecettes(u):\n",
    "    return u[0:int((2*len(u))/3)]\n",
    "\n",
    "trainrecettes_=F.udf(trainrecettes,ArrayType(StringType()))\n",
    "\n",
    "dftrain=grouped.select('uuid2','count',trainrecettes_('ingredientsrecettes3').alias('ingredientsrecettes'),trainrecettes_('listerecettes3').alias('listerecettes'))\n",
    "\n",
    "#on veut construire une liste contenant tous les mots pertinents de la liste d'ingrédients, en se débarassant \n",
    "#dans un premier temps des quantités, puis des stopwords à l'aide de la commande existante du package NLTK.\n",
    "#On se débarasse également des lettres seules exclues, provenant d'abréviations comme c a s pour 'cuillière à soupe',\n",
    "#g pour grammes, mg, kg etc.\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "list_nb = '0 1 2 3 4 5 6 7 8 9'.split()\n",
    "list_lt='a b c d e f g h i j k l m n o p q r s t u v w kg mg gr cl ml dl cuil'.split()\n",
    "stopwords=StopWordsRemover.loadDefaultStopWords('french')\n",
    "\n",
    "def transform (l):\n",
    "    return [mot for recette in l for mot in re.split(' |;',''.join([i if i not in list_nb else '' for i in recette ])) if mot!='' if mot not in stopwords and mot not in list_lt] \n",
    "\n",
    "transform_df  = F.udf(transform,ArrayType(StringType()))\n",
    "\n",
    "#Nouveau dataFrame auquel on a appliqué la fonction transform\n",
    "dftrain2=dftrain.select('uuid2','listerecettes',transform_df('ingredientsrecettes').alias('ingredientsrecettes2'))\n",
    "\n",
    "\n",
    "#Nous obtenons ici pour chaque individu un vecteur de la taille du corpus des ingrédients des recettes, dont chaque élément \n",
    "#représente la fréquence de l'ingrédient dans les recettes consultées par l'individu\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"ingredientsrecettes2\", outputCol=\"CountVectorizer\")\n",
    "\n",
    "model = cv.fit(dftrain2)\n",
    "\n",
    "result1 = model.transform(dftrain2)\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "def bigvect1(x):\n",
    "    return ([int(i) for i in x])\n",
    "\n",
    "\n",
    "bigvect1_  = F.udf(bigvect1,ArrayType(IntegerType()))\n",
    "\n",
    "resultat1=result1.select('uuid2','listerecettes','ingredientsrecettes2',bigvect1_('CountVectorizer').alias('CountVectorizer2'))\n",
    "resultat1.cache().show(truncate=False)\n",
    "\n",
    "def testrecettes(u):\n",
    "    return u[int((2*len(u))/3):len(u)]\n",
    "\n",
    "testrecettes_=F.udf(testrecettes,ArrayType(StringType()))\n",
    "\n",
    "resultattest=grouped.select('uuid2',testrecettes_('listerecettes3').alias('recettestest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Nous obtenons ici pour chaque individu un vecteur de la taille du corpus des ingrédients des recettes, dont chaque élément \n",
    "#représente le TF-IDF de l'ingrédient dans les recettes consultées par l'individu\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"ingredientsrecettes2\", outputCol=\"features\")\n",
    "model = cv.fit(dftrain2)\n",
    "results = model.transform(dftrain2)\n",
    "\n",
    "\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"tfidf\")\n",
    "idfModel = idf.fit(results)\n",
    "result2 = idfModel.transform(results)\n",
    "\n",
    "result2.select(\"uuid2\", \"tfidf\").show(truncate=False)\n",
    "\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "def bigvect2(x):\n",
    "    return ([round(i,3) for i in x])\n",
    "\n",
    "\n",
    "\n",
    "bigvect2_  = F.udf(bigvect2,ArrayType(FloatType()))\n",
    "\n",
    "resultat2=result2.select('uuid2','listerecettes','ingredientsrecettes2',bigvect2_('tfidf').alias('tfidf2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Recommendations de users à recettes selon la méthode de Jaccard sur les vecteurs des fréquences des mots\n",
    "def recorecettesjaccard(id,df,resultat1):\n",
    "    \n",
    "    #Nous créons les vecteurs des fréquences des ingrédients pour chaque recette de la base train\n",
    "    from pyspark.sql.functions import collect_list\n",
    "    grouped_ = df.groupBy(['title']).agg(collect_list(\"uuid2\").alias('users'),collect_list(\"ingredients\").alias('ingredients2'))\n",
    "    grouped_.head(50)\n",
    "\n",
    "    def double(l):\n",
    "        return list(set(l))\n",
    "\n",
    "    double_=F.udf(double,ArrayType(StringType()))\n",
    "\n",
    "    grouped_=grouped_.select('title','users','ingredients2',double_('ingredients2').alias('ingredients3'))\n",
    "\n",
    "    dfid= resultat1.filter(resultat1.uuid2 ==id )\n",
    "    recettes3=dfid.collect()[0].listerecettes\n",
    "    ingredients3=dfid.collect()[0].ingredientsrecettes2\n",
    "    \n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "    list_nb = '0 1 2 3 4 5 6 7 8 9'.split()\n",
    "    list_lt='a b c d e f g h i j k l m n o p q r s t u v w kg mg gr cl ml dl'.split()\n",
    "    stopwords=StopWordsRemover.loadDefaultStopWords('french')\n",
    "\n",
    "    def transform2 (l):\n",
    "         return [mot for recette in l for mot in re.split(' |;',''.join([i if i not in list_nb else '' for i in recette ])) if mot!='' if mot not in stopwords and mot not in list_lt] \n",
    "\n",
    "    transform2_df  = F.udf(transform2,ArrayType(StringType()))\n",
    "\n",
    "    grouped_=grouped_.select('title','users','ingredients3',transform2_df('ingredients3').alias('ingredients4'))\n",
    "\n",
    "    newRow = spark.createDataFrame([[id,[id],ingredients3,ingredients3]])\n",
    "    grouped_ = grouped_.union(newRow)\n",
    "\n",
    "    from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "    cv = CountVectorizer(inputCol=\"ingredients4\", outputCol=\"CountVectorizer\")\n",
    "\n",
    "    model = cv.fit(grouped_)\n",
    "\n",
    "    result3 = model.transform(grouped_)\n",
    "\n",
    "    from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "    def bigvect3(x):\n",
    "        return ([int(i) for i in x])\n",
    "\n",
    "\n",
    "    bigvect3_  = F.udf(bigvect3,ArrayType(IntegerType()))\n",
    "\n",
    "    resultat3=result3.select('title','users','ingredients4',bigvect3_('CountVectorizer').alias('CountVectorizer2'))\n",
    "\n",
    "    x=resultat3.filter(resultat3.title==id).collect()[0].CountVectorizer2\n",
    "    \n",
    "    #Calcul de l'indice de Jaccard entre le vecteur des fréquences des ingrédients du user en entrée de la fonction \n",
    "    #et les vecteurs des fréquences des ingrédients de chaque recette\n",
    "\n",
    "    def inter(y):\n",
    "        return sum(i+j for i,j in zip(x,y) if i>0 and j>0)\n",
    "\n",
    "    def union(y):\n",
    "        return sum(i+j for i,j in zip(x,y))\n",
    "\n",
    "    def jaccard(y):\n",
    "        return round(float(inter(y))/float(union(y)),3)\n",
    "\n",
    "\n",
    "    jaccard_  = F.udf(jaccard,FloatType())\n",
    "\n",
    "    resultatjac=resultat3.select('title','users','ingredients4','CountVectorizer2',jaccard_('CountVectorizer2').alias('Jaccard'))\n",
    "\n",
    "    resultatjac=resultatjac.filter(resultatjac.title !=id).filter(resultatjac.Jaccard !=1.0 )\n",
    "    for i in recettes3:\n",
    "        resultatjac=resultatjac.filter(resultatjac.title !=i)\n",
    "\n",
    "    from pyspark.sql.functions import desc\n",
    "\n",
    "    resultatjac=resultatjac.sort(desc('Jaccard'))\n",
    "\n",
    "    #Recommendations des 5 recettes aux indices de Jaccard les plus élevés\n",
    "    reco=resultatjac.head(5)\n",
    "\n",
    "    return ([i.title for i in reco])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "identifiants=resultat1.select('uuid2').collect()\n",
    "identifiants=[i.uuid2 for i in identifiants]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluation du modèle de recommendation de users à recettes selon la distance de Jaccard pour les individus de la base\n",
    "dic={}\n",
    "for i in identifiants:\n",
    "    dic.setdefault(i,recorecettesjaccard(i,df2,resultat1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "l = [(x,y) for x,y in zip(dic.keys(),dic.values())]\n",
    "rdd = sc.parallelize(l)\n",
    "rdd2 = rdd.map(lambda x: Row(uuid2=x[0], recettesreco=x[1]))\n",
    "dff = sqlContext.createDataFrame(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultat =resultattest.select('uuid2','recettestest').join(dff,'uuid2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluation de la performance, autrement dit le pourcentage de personnes dont au moins une des 5 recettes recommendées\n",
    "#appartient aux 1/3 de recettes isolées au début pour chaque individu\n",
    "def perf(u):\n",
    "    nb=0.0\n",
    "    for i in u.recettesreco:\n",
    "        if i in u.recettestest:\n",
    "            nb=1.0\n",
    "            break\n",
    "    return u.uuid2,u.recettestest,u.recettesreco,nb\n",
    "\n",
    "\n",
    "l=resultat.rdd.map(lambda row: perf(row)).collect()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calcul de la performance\n",
    "a=[i[3] for i in l]\n",
    "tauxperfjaccard=float(sum(a))/float(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6486486486486487"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tauxperfjaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Recommendations de users à recettes selon la méthode de la similarité cosinus sur les vecteurs des fréquences des mots\n",
    "\n",
    "def recorecettessimicos(id,df,resultat1):\n",
    " \n",
    "    from pyspark.sql.functions import collect_list\n",
    "    df=df.dropDuplicates()\n",
    "    grouped_ = df.groupBy(['title']).agg(collect_list(\"uuid2\").alias('users'),collect_list(\"ingredients\").alias('ingredients2'))\n",
    "\n",
    "\n",
    "    def double(l):\n",
    "        return list(set(l))\n",
    "\n",
    "\n",
    "    double_=F.udf(double,ArrayType(StringType()))\n",
    "\n",
    "    grouped_=grouped_.select('title','users','ingredients2',double_('ingredients2').alias('ingredients3'))\n",
    "\n",
    "    dfid= resultat1.filter(resultat1.uuid2 ==id )\n",
    "    recettes3=dfid.collect()[0].listerecettes\n",
    "    ingredients3=dfid.collect()[0].ingredientsrecettes2\n",
    "\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "    list_nb = '0 1 2 3 4 5 6 7 8 9'.split()\n",
    "    list_lt='a b c d e f g h i j k l m n o p q r s t u v w kg mg gr cl ml dl'.split()\n",
    "    stopwords=StopWordsRemover.loadDefaultStopWords('french')\n",
    "\n",
    "    def transform2 (l):\n",
    "         return [mot for recette in l for mot in re.split(' |;',''.join([i if i not in list_nb else '' for i in recette ])) if mot!='' if mot not in stopwords and mot not in list_lt] \n",
    "\n",
    "    transform2_df  = F.udf(transform2,ArrayType(StringType()))\n",
    "\n",
    "    grouped_=grouped_.select('title','users','ingredients3',transform2_df('ingredients3').alias('ingredients4'))\n",
    "\n",
    "    newRow = spark.createDataFrame([[id,[id],ingredients3,ingredients3]])\n",
    "    grouped_ = grouped_.union(newRow)\n",
    "\n",
    "    from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "    cv = CountVectorizer(inputCol=\"ingredients4\", outputCol=\"CountVectorizer\")\n",
    "\n",
    "    model = cv.fit(grouped_)\n",
    "\n",
    "    result3 = model.transform(grouped_)\n",
    "\n",
    "    from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "    def bigvect3(x):\n",
    "        return ([int(i) for i in x])\n",
    "\n",
    "\n",
    "    bigvect3_  = F.udf(bigvect3,ArrayType(IntegerType()))\n",
    "\n",
    "    resultat3=result3.select('title','users','ingredients4',bigvect3_('CountVectorizer').alias('CountVectorizer2'))\n",
    "\n",
    "    x=resultat3.filter(resultat3.title==id).collect()[0].CountVectorizer2\n",
    "    \n",
    "    #Calcul de la similarité cosinus entre le vecteur des fréquences des ingrédients du user en entrée de la fonction \n",
    "    #et les vecteurs des fréquences des ingrédients de chaque recette\n",
    "\n",
    "    from math import sqrt\n",
    "\n",
    "    def prodscal(y):\n",
    "        return sum(i*j for i,j in zip(x,y))\n",
    "\n",
    "    def longueur(y):\n",
    "        return sum(i*i for i in y)\n",
    "\n",
    "    def cos(y):\n",
    "        try:\n",
    "            return float(prodscal(y))/float((sqrt(longueur(y))*sqrt(longueur(x))))\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "    cos_  = F.udf(cos,FloatType())\n",
    "\n",
    "    resultatcos=resultat3.select('title','users','ingredients4','CountVectorizer2',cos_('CountVectorizer2').alias('cos'))\n",
    "\n",
    "    resultatcos=resultatcos.filter(resultatcos.title != id)\n",
    "\n",
    "\n",
    "    for i in recettes3:\n",
    "        resultatcos=resultatcos.filter(resultatcos.title != i)\n",
    "\n",
    "    from pyspark.sql.functions import desc\n",
    "\n",
    "    resultatcos=resultatcos.sort(desc('cos'))\n",
    "    \n",
    "    #Recommendation des 5 recettes aux similarités cosinus les plus élevées\n",
    "    reco=resultatcos.head(5)\n",
    "    return ([i.title for i in reco])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluation du modèle de recommendation de users à users, puis de users à recettes selon la distance de Jaccard \n",
    "#pour les individus de la base\n",
    "dic2={}\n",
    "for i in identifiants:\n",
    "    dic2.setdefault(i,recorecettessimicos(i,df2,resultat1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "l = [(x,y) for x,y in zip(dic2.keys(),dic2.values())]\n",
    "rdd = sc.parallelize(l)\n",
    "rdd2 = rdd.map(lambda x: Row(uuid2=x[0], recettesreco=x[1]))\n",
    "dff = sqlContext.createDataFrame(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultat =resultattest.select('uuid2','recettestest').join(dff,'uuid2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation de la performance, autrement dit le pourcentage de personnes dont au moins une des 5 recettes recommendées\n",
    "#appartient aux 1/3 de recettes isolées au début pour chaque individu\n",
    "def perf(u):\n",
    "    nb=0;0\n",
    "    for i in u.recettesreco:\n",
    "        if i in u.recettestest:\n",
    "            nb=1.0\n",
    "            break\n",
    "    return u.uuid2,u.recettestest,u.recettesreco,nb\n",
    "\n",
    "\n",
    "l=resultat.rdd.map(lambda row: perf(row)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[i[3] for i in l]\n",
    "tauxperfsimicos=float(sum(a))/float(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calcul de la performance\n",
    "tauxperfsimicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Recommendations de users à recettes selon la méthode de la similarité cosinus sur les vecteurs des TF-IDF des ingrédients \n",
    "def recorecettestfidfcos(id,df,resultat2):\n",
    "    \n",
    "    from pyspark.sql.functions import collect_list\n",
    "    df=df.dropDuplicates()\n",
    "    grouped_ = df.groupBy(['title']).agg(collect_list(\"uuid2\").alias('users'),collect_list(\"ingredients\").alias('ingredients2'))\n",
    "\n",
    "\n",
    "    def double(l):\n",
    "        return list(set(l))\n",
    "\n",
    "\n",
    "    double_=F.udf(double,ArrayType(StringType()))\n",
    "\n",
    "    grouped_=grouped_.select('title','users','ingredients2',double_('ingredients2').alias('ingredients3'))\n",
    "\n",
    "    dfid= resultat2.filter(resultat2.uuid2 ==id )\n",
    "    recettes3=dfid.collect()[0].listerecettes\n",
    "    ingredients3=dfid.collect()[0].ingredientsrecettes2\n",
    "\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "    list_nb = '0 1 2 3 4 5 6 7 8 9'.split()\n",
    "    list_lt='a b c d e f g h i j k l m n o p q r s t u v w kg mg gr cl ml dl'.split()\n",
    "    stopwords=StopWordsRemover.loadDefaultStopWords('french')\n",
    "\n",
    "    def transform2 (l):\n",
    "         return [mot for recette in l for mot in re.split(' |;',''.join([i if i not in list_nb else '' for i in recette ])) if mot!='' if mot not in stopwords and mot not in list_lt] \n",
    "\n",
    "    transform2_df  = F.udf(transform2,ArrayType(StringType()))\n",
    "\n",
    "    grouped_=grouped_.select('title','users','ingredients3',transform2_df('ingredients3').alias('ingredients4'))\n",
    "\n",
    "    newRow = spark.createDataFrame([[id,[id],ingredients3,ingredients3]])\n",
    "    grouped_ = grouped_.union(newRow)\n",
    "\n",
    "    from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "    from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "    cv = CountVectorizer(inputCol=\"ingredients4\", outputCol=\"features\")\n",
    "    model = cv.fit(grouped_ )\n",
    "    results = model.transform(grouped_ )\n",
    "\n",
    "    #Nous obtenons ici pour chaque individu un vecteur de la taille du corpus des ingrédients des recettes, dont chaque élément \n",
    "    #représente le TF-IDF de l'ingrédient dans les recettes consultées par l'individu\n",
    "    \n",
    "    idf = IDF(inputCol=\"features\", outputCol=\"tfidf\")\n",
    "    idfModel = idf.fit(results)\n",
    "    result2 = idfModel.transform(results)\n",
    "\n",
    "\n",
    "    from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "    def bigvect2(x):\n",
    "        return ([round(i,3) for i in x])\n",
    "\n",
    "\n",
    "\n",
    "    bigvect2_  = F.udf(bigvect2,ArrayType(FloatType()))\n",
    "\n",
    "    resultat2=result2.select('title','userss','ingredients4',bigvect2_('tfidf').alias('tfidf2'))\n",
    "\n",
    "\n",
    "    x=resultat2.filter(resultat2.title==id).collect()[0].tfidf2\n",
    "    \n",
    "    #Calcul de l'indice de la similarité cosinus entre le vecteur des TF-IDF des ingrédients du user en entrée de la fonction \n",
    "    #et les vecteurs des TF-IDF des ingrédients de chaque recette\n",
    "\n",
    "    from math import sqrt\n",
    "\n",
    "    def prodscal(y):\n",
    "        return sum(i*j for i,j in zip(x,y))\n",
    "\n",
    "    def longueur(y):\n",
    "        return sum(i*i for i in y)\n",
    "\n",
    "    def cos(y):\n",
    "        try:\n",
    "            return float(prodscal(y))/float((sqrt(longueur(y))*sqrt(longueur(x))))\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "    cos_  = F.udf(cos,FloatType())\n",
    "\n",
    "    resultatcos=resultat2.select('title','users','ingredients4','tfidf2',cos_('tfidf2').alias('cos'))\n",
    "\n",
    "    resultatcos=resultatcos.filter(resultatcos.title != id)\n",
    "\n",
    "\n",
    "    for i in recettes3:\n",
    "        resultatcos=resultatcos.filter(resultatcos.title != i)\n",
    "\n",
    "    from pyspark.sql.functions import desc\n",
    "\n",
    "    resultatcos=resultatcos.sort(desc('cos'))\n",
    "\n",
    "    reco=resultatcos.head(5)\n",
    "    \n",
    "    #Recommendations de 5 recettes\n",
    "    return ([i.title for i in reco])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic4={}\n",
    "for i in identifiants:\n",
    "    dic4.setdefault(i,recorecettestfidfjac(i,df2,resultat1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "l = [(x,y) for x,y in zip(dic4.keys(),dic4.values())]\n",
    "rdd = sc.parallelize(l)\n",
    "rdd2 = rdd.map(lambda x: Row(uuid2=x[0], recettesreco=x[1]))\n",
    "dff = sqlContext.createDataFrame(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultat =resultattest.select('uuid2','recettestest').join(dff,'uuid2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perf(u):\n",
    "    nb=0;0\n",
    "    for i in u.recettesreco:\n",
    "        if i in u.recettestest:\n",
    "            nb=1.0\n",
    "            break\n",
    "    return u.uuid2,u.recettestest,u.recettesreco,nb\n",
    "\n",
    "\n",
    "l=resultat.rdd.map(lambda row: perf(row)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[i[3] for i in l]\n",
    "tauxperftfidfcos=float(sum(a))/float(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calcul de la performance\n",
    "tauxperftfidfcos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Recommendation de users à users, puis de users à recettes selon l'indice de Jaccard sur les vecteurs des fréquences\n",
    "#des ingrédients\n",
    "#On cherche les 5 individus les plus proches de l'individu en entrée, et on recommende 5 recettes visionnées par ces individus et \n",
    "#différentes de celles visitées par le user en entrée, dans l'ordre de pertinence des individus\n",
    "\n",
    "def recorecettesjacusers(id,resultat1):\n",
    "    \n",
    "    dfid= resultat1.filter(resultat1.uuid2 ==id )\n",
    "    x=dfid.collect()[0].CountVectorizer2\n",
    "    recettes1=dfid.collect()[0].listerecettes\n",
    "\n",
    "    #On calcule l'indice de Jaccard entre le vecteur des fréquences des ingrédients du user en entrée et les autres users de la base train\n",
    "    def inter(y):\n",
    "        return sum(i+j for i,j in zip(x,y) if i>0 and j>0)\n",
    "\n",
    "    def union(y):\n",
    "        return sum(i+j for i,j in zip(x,y))\n",
    "\n",
    "    def jaccard(y):\n",
    "        return round(float(inter(y))/float(union(y)),3)\n",
    "\n",
    "\n",
    "    jaccard_  = F.udf(jaccard,FloatType())\n",
    "\n",
    "\n",
    "    resultat_=resultat1.select('uuid2','listerecettes','ingredientsrecettes2','CountVectorizer2',jaccard_('CountVectorizer2').alias('Jaccard'))\n",
    "\n",
    "    resultat_=resultat_.filter(resultat_.uuid2 != id)\n",
    "\n",
    "\n",
    "    def remove(l):\n",
    "        return [i for i in l if i not in recettes1]\n",
    "\n",
    "    remove_  = F.udf(remove,ArrayType(StringType()))\n",
    "\n",
    "\n",
    "    resultat_=resultat_.select('uuid2',remove_('listerecettes').alias('listerecettes2'),'ingredientsrecettes2','CountVectorizer2','Jaccard')\n",
    "\n",
    "\n",
    "    resultatreco=resultat_.dropDuplicates(['listerecettes2'])      \n",
    "\n",
    "\n",
    "    from pyspark.sql.functions import desc\n",
    "\n",
    "    resultat_=resultat_.sort(desc('Jaccard'))\n",
    "    resultatreco=resultatreco.sort(desc('Jaccard'))\n",
    "    \n",
    "    reco=[i.listerecettes2 for i in resultatreco.head(5)]\n",
    "    ingredients=[i.ingredientsrecettes2 for i in resultatreco.head(5)]\n",
    "    \n",
    "    reco2=[i.uuid2 for i in resultat_.head(5)]\n",
    "    \n",
    "    return ([i for l in reco for i in l ][:5],reco2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic5={}\n",
    "for i in identifiants:\n",
    "    dic5.setdefault(i,recorecettesjacusers(i,resultat1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "l = [(x,y) for x,y in zip(dic5.keys(),dic5.values())]\n",
    "rdd = sc.parallelize(l)\n",
    "rdd2 = rdd.map(lambda x: Row(uuid2=x[0], recettesreco=x[1][0],usersreco=x[1][1]))\n",
    "dff = sqlContext.createDataFrame(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultat =resultattest.select('uuid2','recettestest').join(dff,'uuid2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultat.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perf(u):\n",
    "    nb=0;0\n",
    "    for i in u.recettesreco:\n",
    "        if i in u.recettestest:\n",
    "            nb=1.0\n",
    "            break\n",
    "    return u.uuid2,u.recettestest,u.recettesreco,nb,u.usersreco\n",
    "\n",
    "\n",
    "l=resultat.rdd.map(lambda row: perf(row)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calcul de la performance\n",
    "a=[i[3] for i in l]\n",
    "m=[len(v) for i in l  for v in i[4]]\n",
    "n=[len(v) for i in l  for v in i[5]]\n",
    "tauxperfjacusers=float(sum(a))/float(len(a))\n",
    "nbrecettesmoyenjacusers=float(sum(m))/float(len(m))\n",
    "nbingredientsmoyenjacusers=float(sum(n))/float(len(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tauxperfjacusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbrecettesmoyenjacusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbingredientsmoyenjacusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Recommendation de users à users, puis de users à recettes selon l'indice de la similarité cosinus sur les vecteurs des fréquences\n",
    "#des ingrédients\n",
    "#On cherche les 5 individus les plus proches de l'individu en entrée, et on recommende 5 recettes visionnées par ces individus et \n",
    "#différentes de celles visitées par le user en entrée, dans l'ordre de pertinence des individus\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "def recorecettescosusers(id,resultat1):\n",
    "    \n",
    "    dfid= resultat1.filter(resultat1.uuid2 ==id )\n",
    "    x=dfid.collect()[0].CountVectorizer2\n",
    "    recettes=dfid.collect()[0].listerecettes\n",
    "\n",
    "\n",
    "    def prodscal(y):\n",
    "        return sum(i*j for i,j in zip(x,y))\n",
    "\n",
    "    def longueur(y):\n",
    "        return sum(i*i for i in y)\n",
    "\n",
    "    def cos(y):\n",
    "        try:\n",
    "            return float(prodscal(y))/float((sqrt(longueur(y))*sqrt(longueur(x))))\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "    cos_  = F.udf(cos,FloatType())\n",
    "\n",
    "    resultat_=resultat1.select('uuid2','listerecettes','ingredientsrecettes2','CountVectorizer2',cos_('CountVectorizer2').alias('cos'))\n",
    "\n",
    "    resultat_=resultat_.filter(resultat_.uuid2 != id)\n",
    "    \n",
    "    def remove(l):\n",
    "        return [i for i in l if i not in recettes]\n",
    "\n",
    "    remove_  = F.udf(remove,ArrayType(StringType()))\n",
    "\n",
    "    resultat_=resultat_.select('uuid2',remove_('listerecettes').alias('listerecettes2'),'ingredientsrecettes2','CountVectorizer2','cos')\n",
    "\n",
    "    resultatreco=resultat_.dropDuplicates(['listerecettes2'])  \n",
    "\n",
    "    from pyspark.sql.functions import desc\n",
    "\n",
    "    resultat_=resultat_.sort(desc('cos'))\n",
    "    resultatreco=resultatreco.sort(desc('cos'))\n",
    "    \n",
    "    reco=[i.listerecettes2 for i in resultatreco.head(5)]\n",
    "    \n",
    "    reco2=[i.uuid2 for i in resultat_.head(5)]\n",
    "    \n",
    "    return ([i for l in reco for i in l ][:5],reco2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic6={}\n",
    "for i in identifiants:\n",
    "    dic6.setdefault(i,recorecettescosusers(i,resultat1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "l = [(x,y) for x,y in zip(dic6.keys(),dic6.values())]\n",
    "rdd = sc.parallelize(l)\n",
    "rdd2 = rdd.map(lambda x: Row(uuid2=x[0], recettesreco=x[1][0],usersreco=x[1][1]))\n",
    "dff = sqlContext.createDataFrame(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dff.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultat =resultattest.select('uuid2','recettestest').join(dff,'uuid2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultat.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perf(u):\n",
    "    nb=0;0\n",
    "    for i in u.recettesreco:\n",
    "        if i in u.recettestest:\n",
    "            nb=1.0\n",
    "            break\n",
    "    return u.uuid2,u.recettestest,u.recettesreco,nb,u.usersreco\n",
    "\n",
    "\n",
    "l=resultat.rdd.map(lambda row: perf(row)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calcul de la performance\n",
    "a=[i[3] for i in l]\n",
    "m=[len(v) for i in l  for v in i[4]]\n",
    "n=[len(v) for i in l  for v in i[5]]\n",
    "tauxperfcosusers=float(sum(a))/float(len(a))\n",
    "nbrecettesmoyencosusers=float(sum(m))/float(len(m))\n",
    "nbingredientsmoyencosusers=float(sum(m))/float(len(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tauxperfcosusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbrecettesmoyencosusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbingredientsmoyencosusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Recommendation de users à users, puis de users à recettes selon l'indice de la similarité cosinus sur les vecteurs des TF-IDF\n",
    "#des ingrédients\n",
    "#On cherche les 5 individus les plus proches de l'individu en entrée, et on recommende 5 recettes visionnées par ces individus et \n",
    "#différentes de celles visitées par le user en entrée, dans l'ordre de pertinence des individus\n",
    "\n",
    "def recorecettestfidfuserscos(id,resultat2):\n",
    "\n",
    "    # On fait la recommendation en prenant en entrée un user précis,et en utilisant la similarité cosinus sur les vecteurs des TF-IDF \n",
    "\n",
    "    dfid= resultat2.filter(resultat2.uuid2 ==id )\n",
    "    x=dfid.collect()[0].tfidf2\n",
    "    recettes2=dfid.collect()[0].listerecettes\n",
    "\n",
    "    from math import sqrt\n",
    "\n",
    "    def prodscal(y):\n",
    "        return sum(i*j for i,j in zip(x,y))\n",
    "\n",
    "    def longueur(y):\n",
    "        return sum(i*i for i in y)\n",
    "\n",
    "    def cos(y):\n",
    "        try:\n",
    "            return prodscal(y)/(sqrt(longueur(y))*sqrt(longueur(x)))\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "    cos_  = F.udf(cos,FloatType())\n",
    "\n",
    "    resultat__=resultat2.select('uuid2','listerecettes','ingredientsrecettes2','tfidf2',cos_('tfidf2').alias('cos'))\n",
    "\n",
    "    resultat__=resultat__.filter(resultat__.uuid2 != id)\n",
    "\n",
    "    def remove(l):\n",
    "        return [i for i in l if i not in recettes2]\n",
    "\n",
    "    remove_  = F.udf(remove,ArrayType(StringType()))\n",
    "\n",
    "    resultatreco=resultat__.select('uuid2',remove_('listerecettes').alias('listerecettes2'),'ingredientsrecettes2','tfidf2','cos')\n",
    "\n",
    "    resultatreco=resultatreco.dropDuplicates(['listerecettes2'])\n",
    "\n",
    "\n",
    "    from pyspark.sql.functions import desc\n",
    "\n",
    "    resultat__=resultat__.sort(desc('cos'))\n",
    "    resultatreco=resultatreco.sort(desc('cos'))\n",
    "\n",
    "    reco=[i.listerecettes2 for i in resultatreco.head(5)]\n",
    "    \n",
    "    reco2=[i.uuid2 for i in resultat_.head(5)]\n",
    "    \n",
    "    return ([i for l in reco for i in l ][:5],reco2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic8={}\n",
    "for i in identifiants:\n",
    "    dic8.setdefault(i,recorecettestfidfuserscos(i,resultat2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "l = [(x,y) for x,y in zip(dic8.keys(),dic8.values())]\n",
    "rdd = sc.parallelize(l)\n",
    "rdd2 = rdd.map(lambda x: Row(uuid2=x[0], recettesreco=x[1][0],usersreco=x[1][1]))\n",
    "dff = sqlContext.createDataFrame(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultat =resultattest.select('uuid2','recettestest').join(dff,'uuid2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perf(u):\n",
    "    nb=0;0\n",
    "    for i in u.recettesreco:\n",
    "        if i in u.recettestest:\n",
    "            nb=1.0\n",
    "            break\n",
    "    return u.uuid2,u.recettestest,u.recettesreco,nb,u.usersreco\n",
    "\n",
    "\n",
    "l=resultat.rdd.map(lambda row: perf(row)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calcul de la performance\n",
    "a=[i[3] for i in l]\n",
    "tauxperftfidfuserscos=float(sum(a))/float(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tauxperftfidfuserscos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#On cherche à savoir quelles sont les 5 recettes les plus vues que l'on recommendera à tous les individus\n",
    "from pyspark.sql.functions import collect_list\n",
    "df2=df2.dropDuplicates()\n",
    "grouped_ = df.groupBy(['title']).agg(collect_list(\"uuid2\").alias('users'),collect_list(\"ingredients\").alias('ingredients2'))\n",
    "\n",
    "def count(u):\n",
    "    return float(len(u))\n",
    "\n",
    "count_=F.udf(count,FloatType())\n",
    "\n",
    "\n",
    "groupedmostseen = grouped_ .select('users','ingredients2','title',count_('users').alias('visites'))\n",
    "groupedmostseen= groupedmostseen.sort(desc('visites'))\n",
    "groupedmostseen.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recomostseen=[ i.title for i in groupedmostseen.select('title').head(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic5={}\n",
    "for i in identifiants:\n",
    "    dic5.setdefault(i,recomostseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "l = [(x,y) for x,y in zip(dic5.keys(),dic5.values())]\n",
    "rdd = sc.parallelize(l)\n",
    "rdd2 = rdd.map(lambda x: Row(uuid2=x[0], recettesreco=x[1]))\n",
    "dff = sqlContext.createDataFrame(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultat =resultattest.select('uuid2','recettestest').join(dff,'uuid2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perf(u):\n",
    "    nb=0;0\n",
    "    for i in u.recettesreco:\n",
    "        if i in u.recettestest:\n",
    "            nb=1.0\n",
    "            break\n",
    "    return u.uuid2,u.recettestest,u.recettesreco,nb\n",
    "\n",
    "\n",
    "l=resultat.rdd.map(lambda row: perf(row)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calcul de la performance\n",
    "a=[i[3] for i in l]\n",
    "tauxperfrecorecettesmostseen=float(sum(a))/float(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tauxperfrecorecettesmostseen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
